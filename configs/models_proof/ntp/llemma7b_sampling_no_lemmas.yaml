model: "hosted_vllm/EleutherAI/llemma_7b"
temperature: 0.7
top_p: 0.95
nb_samples: 128
max_total_tokens: 4096
max_generated_tokens: 1024
use_chat_prompt: false
stopwords:
  - "```"
  - "sorry"
  - "\n\n"
n_processes: 20
gen_processes: 5
prompt_context: "FILE_CONTEXT_NO_LEMMAS"
nl_proof_hint: false
api_key: "-"
api_base_url: "http://localhost:8080/v1"
